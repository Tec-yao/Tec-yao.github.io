<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="爬虫学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="python spider">
<meta property="og:url" content="http://yoursite.com/2018/06/04/python-spider/index.html">
<meta property="og:site_name" content="Yao&#39;s hexo">
<meta property="og:description" content="爬虫学习笔记">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-07-04T09:16:16.067Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="python spider">
<meta name="twitter:description" content="爬虫学习笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/06/04/python-spider/"/>





  <title>python spider | Yao's hexo</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yao's hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/04/python-spider/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yao's hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">python spider</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-04T16:43:39+08:00">
                2018-06-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>爬虫学习笔记</p>
<a id="more"></a>
<h3 id="爬虫基础"><a href="#爬虫基础" class="headerlink" title="爬虫基础"></a>爬虫基础</h3><h4 id="爬虫基本原理"><a href="#爬虫基本原理" class="headerlink" title="爬虫基本原理"></a>爬虫基本原理</h4><p>爬虫简单来说就是获取网页并提取和保存信息的自动化程序</p>
<h4 id="Session-和-Cookies"><a href="#Session-和-Cookies" class="headerlink" title="Session 和 Cookies"></a>Session 和 Cookies</h4><p>http作为无状态协议，没有对事务处理没有记忆能力的，所以需要<br>Session 和 Cookies 来保持http的连接状态，Session在服务端，用来保存<br>用户的会话信息，Cookies在客户端，在浏览器访问网页时会自动附带上给服务器，服务器通过识别Cookies并鉴定出是哪一个用户，然后判断用户的登录状态</p>
<h5 id="会话维持"><a href="#会话维持" class="headerlink" title="会话维持"></a>会话维持</h5><p>当客户端第一次请求服务器是，服务器会返回一和Headers中带哟Set-Cookie字段的Response 给客户端，用以标记是哪一个用户，客户端浏览次会把Cookies放到Request Headers一起提交给服务器，Cookies 携带了 Session ID 信息，服务器检查该 Cookies 即可找到对应的 Session 是什么，然后再判断 Session 来以此来辨认用户状态。</p>
<p>所以我们在登录某个网站的时候，登录成功后服务器会告诉客户端设置哪些 Cookies 信息，在后续访问页面时客户端会把 Cookies 发送给服务器，服务器再找到对应的 Session 加以判断，如果 Session 中的某些设置登录状态的变量是有效的，那就证明用户是处于登录状态的，即可返回登录之后才可以查看的网页内容，浏览器进行解析便可以看到了。</p>
<p>反之，如果传给服务器的 Cookies 是无效的，或者 Session 已经过期了，我们将不能继续访问页面，可能会收到错误的 Response 或者跳转到登录页面重新登录。</p>
<p>所以 Cookies 和 Session 需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录会话控制。</p>
<h5 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h5><p>Cookie，取单数形式。它有这么几个属性：</p>
<pre><code>Name，即该 Cookie 的名称。Cookie 一旦创建，名称便不可更改
Value，即该 Cookie 的值。如果值为 Unicode 字符，需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码。
Max Age，即该 Cookie 失效的时间，单位秒，也常和 Expires 一起使用，通过它可以计算出其有效时间。Max Age 如果为正数，则该Cookie 在 Max Age 秒之后失效。如果为负数，则关闭浏览器时Cookie 即失效，浏览器也不会以任何形式保存该 Cookie。
Path，即该 Cookie 的使用路径。如果设置为 /path/，则只有路径为 /path/ 的页面可以访问该 Cookie。如果设置为 /，则本域名下的所有页面都可以访问该 Cookie。
Domain，即可以访问该 Cookie 的域名。例如如果设置为 .zhihu.com，则所有以 zhihu.com，结尾的域名都可以访问该Cookie。
Size字段，即此 Cookie 的大小。
Http字段，即 Cookie 的 httponly 属性。若此属性为 true，则只有在 HTTP Headers 中会带有此 Cookie 的信息，而不能通过 document.cookie 来访问此 Cookie。
Secure，即该 Cookie 是否仅被使用安全协议传输。安全协议。安全协议有 HTTPS，SSL 等，在网络上传输数据之前先将数据加密。默认为 false。
</code></pre><p>以上便是 Cookies 的基本结构。</p>
<h4 id="代理基本定理"><a href="#代理基本定理" class="headerlink" title="代理基本定理"></a>代理基本定理</h4><p>我们在做爬虫的过程中经常会遇到这样的情况，最初爬虫正常运行，正常抓取数据，一切看起来都是那么的美好，然而一杯茶的功夫可能就会出现错误，比如 403 Forbidden，这时候打开网页一看，可能会看到“您的 IP 访问频率太高”这样的提示。出现这样的现象的原因是网站采取了一些反爬虫的措施，比如服务器会检测某个 IP 在单位时间内的请求次数，如果超过了这个阈值，那么会直接拒绝服务，返回一些错误信息，这种情况可以称之为封 IP，于是乎就成功把我们的爬虫禁掉了。</p>
<p>既然服务器检测的是某个 IP 单位时间的请求次数，那么我们借助某种方式来伪装我们的 IP，让服务器识别不出是由我们本机发起的请求，不就可以成功防止封 IP 了吗？</p>
<p>那么在这里一种有效的方式就是使用代理，使用它我们可以成功伪装 IP，避免本机 IP 被封禁的情况，在后文会有详细的代理使用的说明，在这之前我们需要先了解下代理的基本原理，它是怎样实现 IP 伪装的呢？本节就让我们先了解一下代理的概念。</p>
<h5 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h5><p>我们常称呼的代理实际上指的就是代理服务器，英文叫做 Proxy Server，它的功能是代理网络用户去取得网络信息。形象地说，它是网络信息的中转站。在我们正常请求一个网站时，是发送了 Request 给 Web 服务器，Web 服务器把 Response 传回给我们。如果设置了代理服务器，实际上就是在本机和服务器之间搭建了一个桥，此时本机不是直接向 Web 服务器发起请求，而是向代理服务器发出请求， Request 会发送给代理服务器，然后由代理服务器再发送给 Web 服务器，然后由代理服务器再把 Web 服务器返回的 Response 转发给本机，这样我们同样可以正常访问网页，但这个过程 Web 服务器识别出的真实的 IP 就不再是我们本机的 IP 了，就成功实现了 IP 伪装，这就是代理的基本原理。</p>
<h5 id="代理作用"><a href="#代理作用" class="headerlink" title="代理作用"></a>代理作用</h5><p>那么代理有什么作用呢？我们可以简单列举如下：</p>
<pre><code>突破自身 IP 访问限制，访问一些平时不能访问的站点。
访问一些单位或团体内部资源，如使用教育网内地址段免费代理服务器，就可以用于对教育网开放的各类 FTP 下载上传，以及各类资料查询共享等服务。
提高访问速度，通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。
隐藏真实 IP，上网者也可以通过这种方法隐藏自己的 IP，免受攻击，对于爬虫来说，我们用代理就是为了隐藏自身 IP，防止自身的 IP 被封锁。
</code></pre><h3 id="基本库的使用"><a href="#基本库的使用" class="headerlink" title="基本库的使用"></a>基本库的使用</h3><h4 id="使用urllib"><a href="#使用urllib" class="headerlink" title="使用urllib"></a>使用urllib</h4><p>urllib库是它是 Python 内置的 HTTP 请求库，也就是说我们不需要额外安装即可使用，它包含四个模块：</p>
<h5 id="request"><a href="#request" class="headerlink" title="request"></a>request</h5><p>第一个模块 request，它是最基本的 HTTP 请求模块，我们可以用它来模拟发送一请求，就像在浏览器里输入网址然后敲击回车一样，只需要给库方法传入 URL 还有额外的参数，就可以模拟实现这个过程了。</p>
<p>1.urlopen()</p>
<p>urllib.request 模块提供了最基本的构造 HTTP 请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理authenticaton（授权验证），redirections（重定向)，cookies（浏览器Cookies）以及其它内容。</p>
<p>我们来感受一下它的强大之处，以 Python 官网为例，我们来把这个网页抓下来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<p>该方法返回 HTTPResposne 类型的对象<br>它主要包含的方法有 read()、readinto()、getheader(name)、getheaders()、fileno() 等方法和 msg、version、status、reason、debuglevel、closed 等属性。</p>
<p>下面再来一个实例感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</span><br><span class="line">print(response.status)</span><br><span class="line">print(response.getheaders())</span><br><span class="line">print(response.getheader(<span class="string">'Server'</span>))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">200</span></span><br><span class="line">[(<span class="string">'Server'</span>, <span class="string">'nginx'</span>), (<span class="string">'Content-Type'</span>, <span class="string">'text/html; charset=utf-8'</span>), (<span class="string">'X-Frame-Options'</span>, <span class="string">'SAMEORIGIN'</span>), (<span class="string">'X-Clacks-Overhead'</span>, <span class="string">'GNU Terry Pratchett'</span>), (<span class="string">'Content-Length'</span>, <span class="string">'47397'</span>), (<span class="string">'Accept-Ranges'</span>, <span class="string">'bytes'</span>), (<span class="string">'Date'</span>, <span class="string">'Mon, 01 Aug 2016 09:57:31 GMT'</span>), (<span class="string">'Via'</span>, <span class="string">'1.1 varnish'</span>), (<span class="string">'Age'</span>, <span class="string">'2473'</span>), (<span class="string">'Connection'</span>, <span class="string">'close'</span>), (<span class="string">'X-Served-By'</span>, <span class="string">'cache-lcy1125-LCY'</span>), (<span class="string">'X-Cache'</span>, <span class="string">'HIT'</span>), (<span class="string">'X-Cache-Hits'</span>, <span class="string">'23'</span>), (<span class="string">'Vary'</span>, <span class="string">'Cookie'</span>), (<span class="string">'Strict-Transport-Security'</span>, <span class="string">'max-age=63072000; includeSubDomains'</span>)]</span><br><span class="line">nginx</span><br></pre></td></tr></table></figure>
<p>其函数为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=<span class="keyword">None</span>, [timeout, ]*, cafile=<span class="keyword">None</span>, capath=<span class="keyword">None</span>, cadefault=<span class="keyword">False</span>, context=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>2.Request</p>
<p>由上我们知道利用 urlopen() 方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入 Headers 等信息，我们就可以利用更强大的 Request 类来构建一个请求。</p>
<p>首先我们用一个实例来感受一下 Request 的用法：</p>
<p>import urllib.request</p>
<p>request = urllib.request.Request(‘<a href="https://python.org&#39;" target="_blank" rel="noopener">https://python.org&#39;</a>)<br>response = urllib.request.urlopen(request)<br>print(response.read().decode(‘utf-8’))</p>
<p>可以发现，我们依然是用 urlopen() 方法来发送这个请求，只不过这次 urlopen() 方法的参数不再是一个 URL，而是一个 Request 类型的对象，通过构造这个这个数据结构，一方面我们可以将请求独立成一个对象，另一方面可配置参数更加丰富和灵活。</p>
<p>下面我们看一下 Request 都可以通过怎样的参数来构造，它的构造方法如下：</p>
<p>class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)</p>
<p>第一个 url 参数是请求 URL，这个是必传参数，其他的都是可选参数。</p>
<p>第二个 data 参数如果要传必须传 bytes（字节流）类型的，如果是一个字典，可以先用 urllib.parse 模块里的 urlencode() 编码。</p>
<p>第三个 headers 参数是一个字典，这个就是 Request Headers 了，你可以在构造 Request 时通过 headers 参数直接构造，也可以通过调用 Request 实例的 add_header() 方法来添加。</p>
<p>添加 Request Headers 最常用的用法就是通过修改 User-Agent 来伪装浏览器，默认的 User-Agent 是 Python-urllib，我们可以通过修改它来伪装浏览器，比如要伪装火狐浏览器，你可以把它设置为：</p>
<p>Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11</p>
<p>第四个 origin_req_host 参数指的是请求方的 host 名称或者 IP 地址。</p>
<p>第五个 unverifiable 参数指的是这个请求是否是无法验证的，默认是False。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个 HTML 文档中的图片，但是我们没有自动抓取图像的权限，这时 unverifiable 的值就是 True。</p>
<p>第六个 method 参数是一个字符串，它用来指示请求使用的方法，比如GET，POST，PUT等等。</p>
<p>下面我们传入多个参数构建一个 Request 来感受一下：</p>
<p>from urllib import request, parse</p>
<p>url = ‘<a href="http://httpbin.org/post&#39;" target="_blank" rel="noopener">http://httpbin.org/post&#39;</a><br>headers = {<br>    ‘User-Agent’: ‘Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)’,<br>    ‘Host’: ‘httpbin.org’<br>}<br>dict = {<br>    ‘name’: ‘Germey’<br>}<br>data = bytes(parse.urlencode(dict), encoding=’utf8’)<br>req = request.Request(url=url, data=data, headers=headers, method=’POST’)<br>response = request.urlopen(req)<br>print(response.read().decode(‘utf-8’))</p>
<p>在这里我们通过四个参数构造了一个 Request，url 即请求 URL，在headers 中指定了 User-Agent 和 Host，传递的参数 data 用了 urlencode() 和 bytes() 方法来转成字节流，另外指定了请求方式为 POST。</p>
<p>运行结果如下：</p>
<p>{<br>  “args”: {},<br>  “data”: “”,<br>  “files”: {},<br>  “form”: {<br>    “name”: “Germey”<br>  },<br>  “headers”: {<br>    “Accept-Encoding”: “identity”,<br>    “Content-Length”: “11”,<br>    “Content-Type”: “application/x-www-form-urlencoded”,<br>    “Host”: “httpbin.org”,<br>    “User-Agent”: “Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)”<br>  },<br>  “json”: null,<br>  “origin”: “219.224.169.11”,<br>  “url”: “<a href="http://httpbin.org/post&quot;" target="_blank" rel="noopener">http://httpbin.org/post&quot;</a><br>}</p>
<p>通过观察结果可以发现，我们成功设置了 data，headers 以及 method。</p>
<p>另外 headers 也可以用 add_header() 方法来添加。</p>
<p>req = request.Request(url=url, data=data, method=’POST’)<br>req.add_header(‘User-Agent’, ‘Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)’)</p>
<p>如此一来，我们就可以更加方便地构造一个 Request，实现请求的发送啦。</p>
<p>3.自定义open</p>
<p>简而言之我们可以把它理解为各种处理器，有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的，利用它们我们几乎可以做到任何 HTTP 请求中所有的事情。</p>
<p>首先介绍下 urllib.request 模块里的 BaseHandler类，它是所有其他 Handler 的父类，它提供了最基本的 Handler 的方法，例如 default_open()、protocol_request() 方法等。</p>
<p>接下来就有各种 Handler 子类继承这个 BaseHandler 类，举例几个如下：</p>
<pre><code>HTTPDefaultErrorHandler 用于处理 HTTP 响应错误，错误都会抛出 HTTPError 类型的异常。
HTTPRedirectHandler 用于处理重定向。
HTTPCookieProcessor 用于处理 Cookies。
ProxyHandler 用于设置代理，默认代理为空。
HTTPPasswordMgr 用于管理密码，它维护了用户名密码的表。
HTTPBasicAuthHandler 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。
</code></pre><p>另外一个比较重要的类就是 OpenerDirector，我们可以称之为 Opener，我们之前用过 urlopen() 这个方法，实际上它就是 Urllib为我们提供的一个 Opener。</p>
<p>那么为什么要引入 Opener 呢？因为我们需要实现更高级的功能，之前我们使用的 Request、urlopen() 相当于类库为你封装好了极其常用的请求方法，利用它们两个我们就可以完成基本的请求，但是现在不一样了，我们需要实现更高级的功能，所以我们需要深入一层进行配置，使用更底层的实例来完成我们的操作。</p>
<p>所以，在这里我们就用到了比调用 urlopen() 的对象的更普遍的对象，也就是 Opener。</p>
<p>Opener 可以使用 open() 方法，返回的类型和 urlopen() 如出一辙。那么它和 Handler 有什么关系？简而言之，就是利用 Handler 来构建 Opener。</p>
<p>下面我们用几个实例来感受一下他们的用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">username = <span class="string">'username'</span></span><br><span class="line">password = <span class="string">'password'</span></span><br><span class="line">url = <span class="string">'http://localhost:5000/'</span></span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="keyword">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>Cookies</p>
<p>Cookies 的处理就需要 Cookies 相关的 Handler 了。</p>
<p>我们先用一个实例来感受一下怎样将网站的 Cookies 获取下来，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">"="</span>+item.value)</span><br></pre></td></tr></table></figure>
<p>首先我们必须声明一个 CookieJar 对象，接下来我们就需要利用 HTTPCookieProcessor 来构建一个 Handler，最后利用 build_opener() 方法构建出 Opener，执行 open() 函数即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">'cookies.txt'</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="keyword">True</span>, ignore_expires=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>这时的 CookieJar就需要换成 MozillaCookieJar，生成文件时需要用到它，它是 CookieJar 的子类，可以用来处理 Cookies 和文件相关的事件，读取和保存 Cookies，它可以将 Cookies 保存成 Mozilla 型浏览器的 Cookies 的格式。</p>
<p>运行之后可以发现生成了一个 cookies.txt 文件。</p>
<p>内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Netscape HTTP Cookie File</span></span><br><span class="line"><span class="comment"># http://curl.haxx.se/rfc/cookie_spec.html</span></span><br><span class="line"><span class="comment"># This is a generated file!  Do not edit.</span></span><br><span class="line"></span><br><span class="line">.baidu.com    TRUE    /    FALSE    <span class="number">3622386254</span>    BAIDUID    <span class="number">05</span>AE39B5F56C1DEC474325CDA522D44F:FG=<span class="number">1</span></span><br><span class="line">.baidu.com    TRUE    /    FALSE    <span class="number">3622386254</span>    BIDUPSID    <span class="number">05</span>AE39B5F56C1DEC474325CDA522D44F</span><br><span class="line">.baidu.com    TRUE    /    FALSE        H_PS_PSSID    <span class="number">19638</span>_1453_17710_18240_21091_18560_17001_21191_21161</span><br><span class="line">.baidu.com    TRUE    /    FALSE    <span class="number">3622386254</span>    PSTM    <span class="number">1474902606</span></span><br><span class="line">www.baidu.com    FALSE    /    FALSE        BDSVRTM    <span class="number">0</span></span><br><span class="line">www.baidu.com    FALSE    /    FALSE        BD_HOME    <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>另外还有一个 LWPCookieJar，同样可以读取和保存 Cookies，但是保存的格式和 MozillaCookieJar 的不一样，它会保存成与 libwww-perl(LWP) 的 Cookies 文件格式。</p>
<p>要保存成 LWP 格式的 Cookies 文件，可以在声明时就改为：</p>
<p>cookie = http.cookiejar.LWPCookieJar(filename)</p>
<p>生成的内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#LWP-Cookies-2.0</span></span><br><span class="line">Set-Cookie3: BAIDUID=<span class="string">"0CE9C56F598E69DB375B7C294AE5C591:FG=1"</span>; path=<span class="string">"/"</span>; domain=<span class="string">".baidu.com"</span>; path_spec; domain_dot; expires=<span class="string">"2084-10-14 18:25:19Z"</span>; version=<span class="number">0</span></span><br><span class="line">Set-Cookie3: BIDUPSID=<span class="number">0</span>CE9C56F598E69DB375B7C294AE5C591; path=<span class="string">"/"</span>; domain=<span class="string">".baidu.com"</span>; path_spec; domain_dot; expires=<span class="string">"2084-10-14 18:25:19Z"</span>; version=<span class="number">0</span></span><br><span class="line">Set-Cookie3: H_PS_PSSID=<span class="number">20048</span>_1448_18240_17944_21089_21192_21161_20929; path=<span class="string">"/"</span>; domain=<span class="string">".baidu.com"</span>; path_spec; domain_dot; discard; version=<span class="number">0</span></span><br><span class="line">Set-Cookie3: PSTM=<span class="number">1474902671</span>; path=<span class="string">"/"</span>; domain=<span class="string">".baidu.com"</span>; path_spec; domain_dot; expires=<span class="string">"2084-10-14 18:25:19Z"</span>; version=<span class="number">0</span></span><br><span class="line">Set-Cookie3: BDSVRTM=<span class="number">0</span>; path=<span class="string">"/"</span>; domain=<span class="string">"www.baidu.com"</span>; path_spec; discard; version=<span class="number">0</span></span><br><span class="line">Set-Cookie3: BD_HOME=<span class="number">0</span>; path=<span class="string">"/"</span>; domain=<span class="string">"www.baidu.com"</span>; path_spec; discard; version=<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>由此看来生成的格式还是有比较大的差异的。</p>
<p>那么生成了 Cookies 文件，怎样从文件读取并利用呢？</p>
<p>下面我们以 LWPCookieJar 格式为例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cookie = http.cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(<span class="string">'cookies.txt'</span>, ignore_discard=<span class="keyword">True</span>, ignore_expires=<span class="keyword">True</span>)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<h5 id="urllib-处理异常"><a href="#urllib-处理异常" class="headerlink" title="urllib 处理异常"></a>urllib 处理异常</h5><p>第二个 error 模块即异常处理模块，如果出现请求错误，我们可以捕获这些异常，然后进行重试或其他操作保证程序不会意外终止。</p>
<p>1.URLError</p>
<p>URLError 类来自 Urllib 库的 error 模块，它继承自 OSError 类，是 error 异常模块的基类，由 request 模块生的异常都可以通过捕获这个类来处理。</p>
<p>它具有一个属性 reason，即返回错误的原因。</p>
<p>下面用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>我们打开一个不存在的页面，照理来说应该会报错，但是这时我们捕获了 URLError 这个异常，运行结果如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Not Found</span><br></pre></td></tr></table></figure></p>
<ol start="2">
<li>HTTPError</li>
</ol>
<p>它是 URLError 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败等等。</p>
<p>它有三个属性。</p>
<pre><code>code，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。
reason，同父类一样，返回错误的原因。
headers，返回 Request Headers。
</code></pre><p>下面我们来用几个实例感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, seq=<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###运行结果：</span></span><br><span class="line"></span><br><span class="line">Not Found</span><br><span class="line"><span class="number">404</span></span><br><span class="line">Server: nginx/<span class="number">1.4</span><span class="number">.6</span> (Ubuntu)</span><br><span class="line">Date: Wed, <span class="number">03</span> Aug <span class="number">2016</span> <span class="number">08</span>:<span class="number">54</span>:<span class="number">22</span> GMT</span><br><span class="line">Content-Type: text/html; charset=UTF<span class="number">-8</span></span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line">Connection: close</span><br><span class="line">X-Powered-By: PHP/<span class="number">5.5</span><span class="number">.9</span><span class="number">-1</span>ubuntu4<span class="number">.14</span></span><br><span class="line">Vary: Cookie</span><br><span class="line">Expires: Wed, <span class="number">11</span> Jan <span class="number">1984</span> <span class="number">05</span>:<span class="number">00</span>:<span class="number">00</span> GMT</span><br><span class="line">Cache-Control: no-cache, must-revalidate, max-age=<span class="number">0</span></span><br><span class="line">Pragma: no-cache</span><br><span class="line">Link: &lt;http://cuiqingcai.com/wp-json/&gt;; rel=<span class="string">"https://api.w.org/"</span></span><br></pre></td></tr></table></figure>
<p>依然是同样的网址，在这里我们捕获了 HTTPError 异常，输出了 reason、code、headers 属性。</p>
<p>因为 URLError 是 HTTPError 的父类，所以我们可以先选择捕获子类的错误，再去捕获父类的错误，所以上述代码更好的写法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Request Successfully'</span>)</span><br></pre></td></tr></table></figure>
<p>这样我们就可以做到先捕获 HTTPError，获取它的错误状态码、原因、Headers 等详细信息。如果非 HTTPError，再捕获 URLError 异常，输出错误原因。最后用 else 来处理正常的逻辑，这是一个较好的异常处理写法。</p>
<p>有时候 reason 属性返回的不一定是字符串，也可能是一个对象，我们再看下面的实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">'https://www.baidu.com'</span>, timeout=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(type(e.reason))</span><br><span class="line">    <span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">'TIME OUT'</span>)</span><br></pre></td></tr></table></figure>
<p>在这里我们直接设置了超时时间来强制抛出 timeout 异常。</p>
<p>运行结果如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">socket</span>.<span class="title">timeout</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">TIME</span> <span class="title">OUT</span></span></span><br></pre></td></tr></table></figure></p>
<p>可以发现 reason 属性的结果是 socket.timeout 类。所以在这里我们可以用 isinstance() 方法来判断它的类型，做出更详细的异常判断。</p>
<p>程序没有直接报错，而是输出了如上内容，这样通过如上操作，我们就可以避免程序异常终止，同时异常得到了有效处理。</p>
<h5 id="解析连接"><a href="#解析连接" class="headerlink" title="解析连接"></a>解析连接</h5><p>第三个 parse 模块是一个工具模块，提供了许多 URL 处理方法，比如拆分、解析、合并等等的方法。</p>
<p>Urllib 库里还提供了 parse 这个模块，它定义了处理 URL 的标准接口，例如实现 URL 各部分的抽取，合并以及链接转换。它支持如下协议的 URL 处理：file、ftp、gopher、hdl、http、https、imap、mailto、 mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、shttp、 sip、sips、snews、svn、svn+ssh、telnet、wais，本节我们介绍一下该模块中常用的方法来感受一下它的便捷之处。</p>
<ol>
<li>urlparse()</li>
</ol>
<p>urlparse() 方法可以实现 URL 的识别和分段，我们先用一个实例来感受一下：</p>
<p>from urllib.parse import urlparse</p>
<p>result = urlparse(‘<a href="http://www.baidu.com/index.html;user?id=5#comment&#39;" target="_blank" rel="noopener">http://www.baidu.com/index.html;user?id=5#comment&#39;</a>)<br>print(type(result), result)</p>
<p>在这里我们利用了 urlparse() 方法进行了一个 URL 的解析，首先输出了解析结果的类型，然后将结果也输出出来。</p>
<p>运行结果：</p>
<p><class 'urllib.parse.parseresult'=""><br>ParseResult(scheme=’http’, netloc=’<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">www.baidu.com&#39;</a>, path=’/index.html’, params=’user’, query=’id=5’, fragment=’comment’)</class></p>
<p>观察可以看到，返回结果是一个 ParseResult 类型的对象，它包含了六个部分，分别是 scheme、netloc、path、params、query、fragment。</p>
<p>观察一下实例的URL：</p>
<p><a href="http://www.baidu.com/index.html;user?id=5#comment" target="_blank" rel="noopener">http://www.baidu.com/index.html;user?id=5#comment</a></p>
<p>urlparse() 方法将其拆分成了六部分，大体观察可以发现，解析时有特定的分隔符，比如 :// 前面的就是 scheme，代表协议，第一个 / 前面便是 netloc，即域名，分号 ; 前面是 params，代表参数。</p>
<p>所以可以得出一个标准的链接格式如下：</p>
<p>scheme://netloc/path;parameters?query#fragment</p>
<p>一个标准的 URL 都会符合这个规则，利用 urlparse() 方法我们可以将它解析拆分开来。</p>
<p>除了这种最基本的解析方式，urlopen() 方法还有其他配置吗？接下来看一下它的 API 用法：</p>
<p>urllib.parse.urlparse(urlstring, scheme=’’, allow_fragments=True)</p>
<p>可以看到它有三个参数：</p>
<pre><code>urlstring，是必填项，即待解析的 URL。
scheme，是默认的协议（比如http、https等），假如这个链接没有带协议信息，会将这个作为默认的协议。
</code></pre><p>我们用一个实例感受一下：</p>
<p>from urllib.parse import urlparse</p>
<p>result = urlparse(‘<a href="http://www.baidu.com/index.html;user?id=5#comment&#39;" target="_blank" rel="noopener">www.baidu.com/index.html;user?id=5#comment&#39;</a>, scheme=’https’)<br>print(result)</p>
<p>运行结果：</p>
<p>ParseResult(scheme=’https’, netloc=’’, path=’<a href="http://www.baidu.com/index.html&#39;" target="_blank" rel="noopener">www.baidu.com/index.html&#39;</a>, params=’user’, query=’id=5’, fragment=’comment’)</p>
<p>可以发现，我们提供的 URL 没有包含最前面的 scheme 信息，但是通过指定默认的 scheme 参数，返回的结果是 https。</p>
<p>假设我们带上了 scheme 呢？</p>
<p>result = urlparse(‘<a href="http://www.baidu.com/index.html;user?id=5#comment&#39;" target="_blank" rel="noopener">http://www.baidu.com/index.html;user?id=5#comment&#39;</a>, scheme=’https’)</p>
<p>结果如下：</p>
<p>ParseResult(scheme=’http’, netloc=’<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">www.baidu.com&#39;</a>, path=’/index.html’, params=’user’, query=’id=5’, fragment=’comment’)</p>
<p>可见 scheme 参数只有在 URL 中不包含 scheme 信息时才会生效，如果 URL 中有 scheme 信息，那就返回解析出的 scheme。</p>
<pre><code>allow_fragments，即是否忽略 fragment，如果它被设置为 False，fragment 部分就会被忽略，它会被解析为 path、parameters 或者 query 的一部分，fragment 部分为空。
</code></pre><p>下面我们用一个实例感受一下：</p>
<p>from urllib.parse import urlparse</p>
<p>result = urlparse(‘<a href="http://www.baidu.com/index.html;user?id=5#comment&#39;" target="_blank" rel="noopener">http://www.baidu.com/index.html;user?id=5#comment&#39;</a>, allow_fragments=False)<br>print(result)</p>
<p>运行结果：</p>
<p>ParseResult(scheme=’http’, netloc=’<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">www.baidu.com&#39;</a>, path=’/index.html’, params=’user’, query=’id=5#comment’, fragment=’’)</p>
<p>假设 URL 中不包含 parameters 和 query 呢？</p>
<p>再来一个实例看下：</p>
<p>from urllib.parse import urlparse</p>
<p>result = urlparse(‘<a href="http://www.baidu.com/index.html#comment&#39;" target="_blank" rel="noopener">http://www.baidu.com/index.html#comment&#39;</a>, allow_fragments=False)<br>print(result)</p>
<p>运行结果：</p>
<p>ParseResult(scheme=’http’, netloc=’<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">www.baidu.com&#39;</a>, path=’/index.html#comment’, params=’’, query=’’, fragment=’’)</p>
<p>可以发现当 URL 中不包含 params 和 query 时， fragment 便会被解析为 path 的一部分。</p>
<p>返回结果 ParseResult 实际上是一个元组，我们可以用索引顺序来获取，也可以用属性名称获取，实例如下：</p>
<p>from urllib.parse import urlparse</p>
<p>result = urlparse(‘<a href="http://www.baidu.com/index.html#comment&#39;" target="_blank" rel="noopener">http://www.baidu.com/index.html#comment&#39;</a>, allow_fragments=False)<br>print(result.scheme, result[0], result.netloc, result[1], sep=’\n’)</p>
<p>在这里我们分别用索引和属性名获取了 scheme 和 netloc，运行结果如下：</p>
<p>http<br>http<br><a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a><br><a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a></p>
<p>可以发现二者结果是一致的，两种方法都可以成功获取。</p>
<ol start="2">
<li>urlunparse()</li>
</ol>
<p>有了 urlparse() 那相应地就有了它的对立方法 urlunparse()。</p>
<p>它接受的参数是一个可迭代对象，但是它的长度必须是 6，否则会抛出参数数量不足或者过多的问题。</p>
<p>先用一个实例感受一下：</p>
<p>from urllib.parse import urlunparse</p>
<p>data = [‘http’, ‘<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">www.baidu.com&#39;</a>, ‘index.html’, ‘user’, ‘a=6’, ‘comment’]<br>print(urlunparse(data))</p>
<p>参数 data 用了列表类型，当然你也可以用其他的类型如元组或者特定的数据结构。</p>
<p>运行结果如下：</p>
<p><a href="http://www.baidu.com/index.html;user?a=6#comment" target="_blank" rel="noopener">http://www.baidu.com/index.html;user?a=6#comment</a></p>
<p>这样我们就成功实现了 URL 的构造。</p>
<ol start="3">
<li>urlsplit()</li>
</ol>
<p>这个和 urlparse() 方法非常相似，只不过它不会单独解析 parameters 这一部分，只返回五个结果。上面例子中的 parameters 会合并到 path中，用一个实例感受一下：</p>
<p>from urllib.parse import urlsplit</p>
<p>result = urlsplit(‘<a href="http://www.baidu.com/index.html;user?id=5#comment&#39;" target="_blank" rel="noopener">http://www.baidu.com/index.html;user?id=5#comment&#39;</a>)<br>print(result)</p>
<p>运行结果：</p>
<p>SplitResult(scheme=’http’, netloc=’<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">www.baidu.com&#39;</a>, path=’/index.html;user’, query=’id=5’, fragment=’comment’)</p>
<p>可以发现返回结果是 SplitResult，其实也是一个元组类型，可以用属性获取值也可以用索引来获取，实例如下：</p>
<p>from urllib.parse import urlsplit</p>
<p>result = urlsplit(‘<a href="http://www.baidu.com/index.html;user?id=5#comment&#39;" target="_blank" rel="noopener">http://www.baidu.com/index.html;user?id=5#comment&#39;</a>)<br>print(result.scheme, result[0])</p>
<p>运行结果：</p>
<p>http http</p>
<ol start="4">
<li>urlunsplit()</li>
</ol>
<p>与 urlunparse() 类似，也是将链接的各个部分组合成完整链接的方法，传入的也是一个可迭代对象，例如列表、元组等等，唯一的区别是，长度必须为 5。</p>
<p>用一个实例来感受一下：</p>
<p>from urllib.parse import urlunsplit</p>
<p>data = [‘http’, ‘<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">www.baidu.com&#39;</a>, ‘index.html’, ‘a=6’, ‘comment’]<br>print(urlunsplit(data))</p>
<p>运行结果：</p>
<p><a href="http://www.baidu.com/index.html?a=6#comment" target="_blank" rel="noopener">http://www.baidu.com/index.html?a=6#comment</a></p>
<p>同样可以完成链接的拼接生成。</p>
<ol start="5">
<li>urljoin()</li>
</ol>
<p>有了 urlunparse() 和 urlunsplit() 方法，我们可以完成链接的合并，不过前提必须要有特定长度的对象，链接的每一部分都要清晰分开。</p>
<p>生成链接还有另一个方法，利用 urljoin() 方法我们可以提供一个 base_url（基础链接），新的链接作为第二个参数，方法会分析 base_url 的 scheme、netloc、path 这三个内容对新链接缺失的部分进行补充，作为结果返回。</p>
<p>我们用几个实例来感受一下：</p>
<p>from urllib.parse import urljoin</p>
<p>print(urljoin(‘<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">http://www.baidu.com&#39;</a>, ‘FAQ.html’))<br>print(urljoin(‘<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">http://www.baidu.com&#39;</a>, ‘<a href="https://cuiqingcai.com/FAQ.html&#39;)" target="_blank" rel="noopener">https://cuiqingcai.com/FAQ.html&#39;)</a>)<br>print(urljoin(‘<a href="http://www.baidu.com/about.html&#39;" target="_blank" rel="noopener">http://www.baidu.com/about.html&#39;</a>, ‘<a href="https://cuiqingcai.com/FAQ.html&#39;)" target="_blank" rel="noopener">https://cuiqingcai.com/FAQ.html&#39;)</a>)<br>print(urljoin(‘<a href="http://www.baidu.com/about.html&#39;" target="_blank" rel="noopener">http://www.baidu.com/about.html&#39;</a>, ‘<a href="https://cuiqingcai.com/FAQ.html?question=2&#39;)" target="_blank" rel="noopener">https://cuiqingcai.com/FAQ.html?question=2&#39;)</a>)<br>print(urljoin(‘<a href="http://www.baidu.com?wd=abc&#39;" target="_blank" rel="noopener">http://www.baidu.com?wd=abc&#39;</a>, ‘<a href="https://cuiqingcai.com/index.php&#39;)" target="_blank" rel="noopener">https://cuiqingcai.com/index.php&#39;)</a>)<br>print(urljoin(‘<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">http://www.baidu.com&#39;</a>, ‘?category=2#comment’))<br>print(urljoin(‘<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">www.baidu.com&#39;</a>, ‘?category=2#comment’))<br>print(urljoin(‘<a href="http://www.baidu.com#comment&#39;" target="_blank" rel="noopener">www.baidu.com#comment&#39;</a>, ‘?category=2’))</p>
<p>运行结果：</p>
<p><a href="http://www.baidu.com/FAQ.html" target="_blank" rel="noopener">http://www.baidu.com/FAQ.html</a><br><a href="https://cuiqingcai.com/FAQ.html" target="_blank" rel="noopener">https://cuiqingcai.com/FAQ.html</a><br><a href="https://cuiqingcai.com/FAQ.html" target="_blank" rel="noopener">https://cuiqingcai.com/FAQ.html</a><br><a href="https://cuiqingcai.com/FAQ.html?question=2" target="_blank" rel="noopener">https://cuiqingcai.com/FAQ.html?question=2</a><br><a href="https://cuiqingcai.com/index.php" target="_blank" rel="noopener">https://cuiqingcai.com/index.php</a><br><a href="http://www.baidu.com?category=2#comment" target="_blank" rel="noopener">http://www.baidu.com?category=2#comment</a><br><a href="http://www.baidu.com?category=2#comment" target="_blank" rel="noopener">www.baidu.com?category=2#comment</a><br><a href="http://www.baidu.com?category=2" target="_blank" rel="noopener">www.baidu.com?category=2</a></p>
<p>可以发现，base_url 提供了三项内容，scheme、netloc、path，如果这三项在新的链接里面不存在，那么就予以补充，如果新的链接存在，那么就使用新的链接的部分。base_url 中的 parameters、query、fragments 是不起作用的。</p>
<p>通过如上的函数，我们可以轻松地实现链接的解析，拼合与生成。</p>
<ol start="6">
<li>urlencode()</li>
</ol>
<p>我们再介绍一个常用的 urlencode() 方法，它在构造 GET 请求参数的时候非常有用，我们用实例感受一下：</p>
<p>from urllib.parse import urlencode</p>
<p>params = {<br>    ‘name’: ‘germey’,<br>    ‘age’: 22<br>}<br>base_url = ‘<a href="http://www.baidu.com?&#39;" target="_blank" rel="noopener">http://www.baidu.com?&#39;</a><br>url = base_url + urlencode(params)<br>print(url)</p>
<p>我们首先声明了一个字典，将参数表示出来，然后调用 urlencode() 方法将其序列化为 URL 标准 GET 请求参数。</p>
<p>运行结果：</p>
<p><a href="http://www.baidu.com?name=germey&amp;age=22" target="_blank" rel="noopener">http://www.baidu.com?name=germey&amp;age=22</a></p>
<p>可以看到参数就成功由字典类型转化为 GET 请求参数了。</p>
<p>这个方法非常常用，有时为了更加方便地构造参数，我们会事先用字典来表示，要转化为 URL 的参数时只需要调用该方法即可。</p>
<ol start="7">
<li>parse_qs()</li>
</ol>
<p>有了序列化必然就有反序列化，如果我们有一串 GET 请求参数，我们利用 parse_qs() 方法就可以将它转回字典，我们用一个实例感受一下：</p>
<p>from urllib.parse import parse_qs</p>
<p>query = ‘name=germey&amp;age=22’<br>print(parse_qs(query))</p>
<p>运行结果：</p>
<p>{‘name’: [‘germey’], ‘age’: [‘22’]}</p>
<p>可以看到这样就成功转回为字典类型了。</p>
<ol start="8">
<li>parse_qsl()</li>
</ol>
<p>另外还有一个 parse_qsl() 方法可以将参数转化为元组组成的列表，实例如下：</p>
<p>from urllib.parse import parse_qsl</p>
<p>query = ‘name=germey&amp;age=22’<br>print(parse_qsl(query))</p>
<p>运行结果：</p>
<p>[(‘name’, ‘germey’), (‘age’, ‘22’)]</p>
<p>可以看到运行结果是一个列表，列表的每一个元素都是一个元组，元组的第一个内容是参数名，第二个内容是参数值。</p>
<ol start="9">
<li>quote()</li>
</ol>
<p>quote() 方法可以将内容转化为 URL 编码的格式，有时候 URL 中带有中文参数的时候可能导致乱码的问题，所以我们可以用这个方法将中文字符转化为 URL 编码，实例如下：</p>
<p>from urllib.parse import quote</p>
<p>keyword = ‘壁纸’<br>url = ‘<a href="https://www.baidu.com/s?wd=&#39;" target="_blank" rel="noopener">https://www.baidu.com/s?wd=&#39;</a> + quote(keyword)<br>print(url)</p>
<p>在这里我们声明了一个中文的搜索文字，然后用 quote() 方法对其进行 URL 编码，最后得到的结果如下：</p>
<p><a href="https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8" target="_blank" rel="noopener">https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8</a></p>
<p>这样我们就可以成功实现URL编码的转换。</p>
<ol start="10">
<li>unquote()</li>
</ol>
<p>有了 quote() 方法当然还有 unquote() 方法，它可以进行 URL 解码，实例如下：</p>
<p>from urllib.parse import unquote</p>
<p>url = ‘<a href="https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8&#39;" target="_blank" rel="noopener">https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8&#39;</a><br>print(unquote(url))</p>
<p>这是上面得到的 URL 编码后的结果，我们在这里利用 unquote() 方法进行还原，结果如下：</p>
<p><a href="https://www.baidu.com/s?wd=壁纸" target="_blank" rel="noopener">https://www.baidu.com/s?wd=壁纸</a></p>
<p>可以看到利用 unquote() 方法可以方便地实现解码。</p>
<h5 id="分析robot协议"><a href="#分析robot协议" class="headerlink" title="分析robot协议"></a>分析robot协议</h5><p>第四个模块是 robotparser，主要是用来识别网站的 robots.txt 文件，然后判断哪些网站可以爬，哪些网站不可以爬的，其实用的比较少。</p>
<p>当爬虫访问一个站点时，首先会检查下这个站点根目录下是否存在 robots.txt 文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，那么搜索爬虫便会访问所有可直接访问的页面。</p>
<p>下面我们看一个 robots.txt 的样例：</p>
<p>User-agent: *<br>Disallow: /<br>Allow: /public/</p>
<p>以上的两行实现了对所有搜索爬虫只允许爬取 public目录的作用。</p>
<p>如上简单的两行，保存成 robots.txt 文件，放在网站的根目录下，和网站的入口文件放在一起。比如 index.php、index.html、index.jsp 等等。</p>
<p>那么上面的 User-agent 就描述了搜索爬虫的名称，在这里将值设置为 *，则代表该协议对任何的爬取爬虫有效。比如我们可以设置：</p>
<p>User-agent: Baiduspider</p>
<p>这就代表我们设置的规则对百度爬虫是有效的。如果有多条 User-agent 记录，则就会有多个爬虫会受到爬取限制，但至少需要指定一条。</p>
<p>Disallow 指定了不允许抓取的目录，比如上述例子中设置为/则代表不允许抓取所有页面。</p>
<p>Allow 一般和 Disallow 一起使用，一般不会单独使用，用来排除某些限制，现在我们设置为 /public/ ，起到的作用是所有页面不允许抓取，但是 public 目录是可以抓取的。</p>
<p>下面我们再来看几个例子感受一下：<br>禁止所有爬虫访问任何目录</p>
<p>User-agent: *<br>Disallow: /</p>
<p>允许所有爬虫访问任何目录</p>
<p>User-agent: *<br>Disallow:</p>
<p>或者直接把 robots.txt 文件留空也是可以的。<br>禁止所有爬虫访问网站某些目录</p>
<p>User-agent: *<br>Disallow: /private/<br>Disallow: /tmp/</p>
<p>只允许某一个爬虫访问</p>
<p>User-agent: WebCrawler<br>Disallow:<br>User-agent: *<br>Disallow: /</p>
<p>以上是 robots.txt 的一些常见写法。</p>
<ol start="2">
<li>爬虫名称</li>
</ol>
<p>大家可能会疑惑，爬虫名是哪儿来的？为什么就叫这个名？其实它是有固定名字的了，比如百度的就叫做 BaiduSpider，下面的表格列出了一些常见的搜索爬虫的名称及对应的网站：<br>爬虫名称     名称     网站<br>BaiduSpider     百度     <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a><br>Googlebot     谷歌     <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>360Spider     360搜索     <a href="http://www.so.com" target="_blank" rel="noopener">www.so.com</a><br>YodaoBot     有道     <a href="http://www.youdao.com" target="_blank" rel="noopener">www.youdao.com</a><br>ia_archiver     Alexa     <a href="http://www.alexa.cn" target="_blank" rel="noopener">www.alexa.cn</a><br>Scooter     altavista     <a href="http://www.altavista.com" target="_blank" rel="noopener">www.altavista.com</a></p>
<ol start="3">
<li>robotparser</li>
</ol>
<p>了解了什么是 Robots 协议之后，我们就可以使用 robotparser 模块来解析 robots.txt 了。</p>
<p>robotparser 模块提供了一个类，叫做 RobotFileParser。它可以根据某网站的 robots.txt 文件来判断一个爬取爬虫是否有权限来爬取这个网页。</p>
<p>使用非常简单，首先看一下它的声明</p>
<p>urllib.robotparser.RobotFileParser(url=’’)</p>
<p>使用这个类的时候非常简单，只需要在构造方法里传入 robots.txt的链接即可。当然也可以声明时不传入，默认为空，再使用 set_url() 方法设置一下也可以。</p>
<p>有常用的几个方法分别介绍一下：</p>
<pre><code>set_url()，用来设置 robots.txt 文件的链接。如果已经在创建 RobotFileParser 对象时传入了链接，那就不需要再使用这个方法设置了。
read()，读取 robots.txt 文件并进行分析，注意这个函数是执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 False，所以一定记得调用这个方法，这个方法不会返回任何内容，但是执行了读取操作。
parse()，用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。
can_fetch()，方法传入两个参数，第一个是 User-agent，第二个是要抓取的 URL，返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 True 或 False。
mtime()，返回的是上次抓取和分析 robots.txt 的时间，这个对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 robots.txt。
modified()，同样的对于长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析 robots.txt 的时间。
</code></pre><p>以上是这个类提供的所有方法，下面我们用实例来感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br></pre></td></tr></table></figure></p>
<p>以简书为例，我们首先创建 RobotFileParser 对象，然后通过 set_url() 方法来设置了 robots.txt 的链接。当然不用这个方法的话，可以在声明时直接用如下方法设置：</p>
<p>rp = RobotFileParser(‘<a href="http://www.jianshu.com/robots.txt&#39;" target="_blank" rel="noopener">http://www.jianshu.com/robots.txt&#39;</a>)</p>
<p>下一步利用了 can_fetch() 方法来判断了网页是否可以被抓取。</p>
<p>运行结果：</p>
<p>True<br>False</p>
<p>同样也可以使用 parser() 方法执行读取和分析。</p>
<p>用一个实例感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.parse(urlopen(<span class="string">'http://www.jianshu.com/robots.txt'</span>).read().decode(<span class="string">'utf-8'</span>).split(<span class="string">'\n'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br></pre></td></tr></table></figure></p>
<p>运行结果一样：</p>
<p>True<br>False</p>
<h4 id="使用requests"><a href="#使用requests" class="headerlink" title="使用requests"></a>使用requests</h4><h5 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h5><h5 id="高级使用"><a href="#高级使用" class="headerlink" title="高级使用"></a>高级使用</h5><p>1.文件上传<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">files = &#123;<span class="string">'file'</span>: open(<span class="string">'favicon.ico'</span>, <span class="string">'rb'</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>, files=files)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></p>
<p>将目录中的文件上传</p>
<p>2.Cookies</p>
<p>‘’’python<br>import requests</p>
<p>headers = {<br>    ‘Cookie’: ‘q_c1=31653b2…’,<br>    ‘Host’: ‘<a href="http://www.zhihu.com&#39;" target="_blank" rel="noopener">www.zhihu.com&#39;</a>,<br>    ‘User-Agent’: ‘Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36’,<br>}<br>r = requests.get(‘<a href="https://www.zhihu.com&#39;" target="_blank" rel="noopener">https://www.zhihu.com&#39;</a>, headers=headers)<br>print(r.text)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cookie的使用也十分方便</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3.会话维持</span><br><span class="line"></span><br><span class="line">在requests中多次使用get()或者post,实际上是相当于不同的会话了。即是不同的Session</span><br><span class="line"></span><br><span class="line">此时利用Session对象可以方便的维护一个会话</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">s = requests.Session()</span><br><span class="line">s.get(&apos;http://httpbin.org/cookies/set/number/123456789&apos;)</span><br><span class="line">r = s.get(&apos;http://httpbin.org/cookies&apos;)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></p>
<p>看下运行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"cookies"</span>: &#123;</span><br><span class="line">    <span class="string">"number"</span>: <span class="string">"123456789"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>4.SLL证书</p>
<p>如果某些网站是使用自己的安全证书，那么获取数据可能会失败<br>这时可以使用verify参数忽略证书</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="keyword">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure>
<p>5.代理设置</p>
<p>对于一些网站而言，当你大规模爬取是，网站可能会验证，甚至把你IP封掉<br>所以需要设置代理，在requests中需要使用proxies参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">'http'</span>: <span class="string">'http://10.10.1.10:3128'</span>,</span><br><span class="line">  <span class="string">'https'</span>: <span class="string">'http://10.10.1.10:1080'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure>
<p>若代理需要使用 HTTP Basic Auth，可以使用类似 <a href="http://user:password@host:port" target="_blank" rel="noopener">http://user:password@host:port</a> 这样的语法来设置代理。</p>
<p>实例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'http://user:password@10.10.1.10:3128/'</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure>
<p>除了基本的 HTTP 代理，Requests 还支持 SOCKS 协议的代理。</p>
<p>除了基本的 HTTP 代理，Requests 还支持 SOCKS 协议的代理。</p>
<p>首先需要安装 Socks 这个库，命令如下：</p>
<p>pip3 install “requests[socks]”</p>
<p>然后就可以使用 SOCKS 协议代理了，实例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://user:password@host:port'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://user:password@host:port'</span></span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure>
<p>6.超时设置</p>
<p>当网络不好时，可能等待时间太长。这时需要超时shezhi</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>, timeout=<span class="number">1</span>)</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure>
<p>7.身份认证</p>
<p>如果遇到这样的网站验证，可以使用 Requests 自带的身份认证功能，实例如下：</p>
<p>import requests<br>from requests.auth import HTTPBasicAuth</p>
<p>r = requests.get(‘<a href="http://localhost:5000&#39;" target="_blank" rel="noopener">http://localhost:5000&#39;</a>, auth=HTTPBasicAuth(‘username’, ‘password’))<br>print(r.status_code)</p>
<p>如果用户名和密码正确的话，请求时就会自动认证成功，会返回 200 状态码，如果认证失败，则会返回 401 状态码。</p>
<p>也可以直接传入元组</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'http://localhost:5000'</span>, auth=(<span class="string">'username'</span>, <span class="string">'password'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure>
<p>它会默认使用 HTTPBasicAuth 这个类来认证</p>
<p>8.Prepared Request</p>
<p>在前面介绍 Urllib 时我们可以将 Request 表示为一个数据结构，Request 的各个参数都可以通过一个 Request 对象来表示，在 Requests 里面同样可以做到，这个数据结构就叫 Prepared Request。</p>
<p>我们用一个实例感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line">s = Session()</span><br><span class="line">req = Request(<span class="string">'POST'</span>, url, data=data, headers=headers)</span><br><span class="line">prepped = s.prepare_request(req)</span><br><span class="line">r = s.send(prepped)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<p>在这里我们引入了 Request，然后用 url、data、headers 参数构造了一个 Request 对象，这时我们需要再调用 Session 的 prepare_request() 方法将其转换为一个 Prepared Request 对象，然后调用 send() 方法发送即可</p>
<h5 id="re模块使用正则表达式"><a href="#re模块使用正则表达式" class="headerlink" title="re模块使用正则表达式"></a>re模块使用正则表达式</h5><p>1.re.macth()</p>
<p>从头开始匹配表达式，返回match类型对象，使用group方法获得结果<br>如果使用了括号，则使用group(0)等获取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 1234567 World_This is a Regex Demo'</span></span><br><span class="line">result = re.match(<span class="string">'^Hello\s(\d+)\sWorld'</span>, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.group(<span class="number">1</span>))</span><br><span class="line">print(result.span())</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;_sre.SRE_Match object; span=(0, 19), match=&apos;Hello 1234567 World&apos;&gt;</span><br><span class="line">Hello 1234567 World</span><br><span class="line">1234567</span><br><span class="line">(0, 19)</span><br></pre></td></tr></table></figure>
<p>匹配的时候可以使用.* 进行通用匹配，它可以匹配除换行符外任意字符<br>但是是贪婪匹配，加上?可以装换为非贪婪匹配</p>
<p>但这里注意，如果匹配的结果在字符串结尾，.*? 就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'http://weibo.com/comment/kEraCN'</span></span><br><span class="line">result1 = re.match(<span class="string">'http.*?comment/(.*?)'</span>, content)</span><br><span class="line">result2 = re.match(<span class="string">'http.*?comment/(.*)'</span>, content)</span><br><span class="line">print(<span class="string">'result1'</span>, result1.group(<span class="number">1</span>))</span><br><span class="line">print(<span class="string">'result2'</span>, result2.group(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result1 </span><br><span class="line">result2 kEraCN</span><br></pre></td></tr></table></figure>
<p>2.修饰符<br>可以选择一些标志修饰符来控制匹配</p>
<p>修饰符     描述<br>re.I    使匹配对大小写不敏感<br>re.L    做本地化识别（locale-aware）匹配<br>re.M    多行匹配，影响 ^ 和 $<br>re.S    使 . 匹配包括换行在内的所有字符<br>re.U    根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B.<br>re.X    该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。</p>
<p>当你需要匹配一些元字符时，需要用到\将元字符装换为普通字符</p>
<p>3.search()</p>
<p>match()函数是从头开始匹配的，而search()函数则是扫描整个字符串，以找到合适的结果，所以为了匹配方便，尽量用search()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = re.search(<span class="string">'&lt;li.*?active.*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>, html, re.S)</span><br><span class="line"><span class="keyword">if</span> result:</span><br><span class="line">    print(result.group(<span class="number">1</span>), result.group(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>由于我们需要获取的歌手和歌名都已经用了小括号包围，所以可以用 group() 方法获取，序号依次对应 group() 的参数。</p>
<p>运行结果：</p>
<p>齐秦 往事随风</p>
<p>4.findall()</p>
<p>使用search()函数只会匹配第一个内容，而使用findall()则返回所有匹配的内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">results = re.findall(<span class="string">'&lt;li.*?href="(.*?)".*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>, html, re.S)</span><br><span class="line">print(results)</span><br><span class="line">print(type(results))</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result)</span><br><span class="line">    print(result[<span class="number">0</span>], result[<span class="number">1</span>], result[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>5.sub()</p>
<p>当你想修改文字或字符串时，使用sub()函数<br>我们用一个实例来感受一下：</p>
<p>import re</p>
<p>content = ‘54aK54yr5oiR54ix5L2g’<br>content = re.sub(‘\d+’, ‘’, content)<br>print(content)</p>
<p>运行结果：aKyroiRixLg</p>
<p>6.compile()</p>
<p>compile()方法将正则字符串编译成正则表达式对象，以便于在后面的匹配中复用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content1 = <span class="string">'2016-12-15 12:00'</span></span><br><span class="line">content2 = <span class="string">'2016-12-17 12:55'</span></span><br><span class="line">content3 = <span class="string">'2016-12-22 13:21'</span></span><br><span class="line">pattern = re.compile(<span class="string">'\d&#123;2&#125;:\d&#123;2&#125;'</span>)</span><br><span class="line">result1 = re.sub(pattern, <span class="string">''</span>, content1)</span><br><span class="line">result2 = re.sub(pattern, <span class="string">''</span>, content2)</span><br><span class="line">result3 = re.sub(pattern, <span class="string">''</span>, content3)</span><br><span class="line">print(result1, result2, result3)</span><br></pre></td></tr></table></figure>
<h3 id="解析库"><a href="#解析库" class="headerlink" title="解析库"></a>解析库</h3><p>提取html信息使用正则表达式，较为繁琐，这时可以使用解析库来解析所获取的网页</p>
<h4 id="XPath"><a href="#XPath" class="headerlink" title="XPath"></a>XPath</h4><p>XPath 全称XML Path Language。即XML路径语言，它是一门在XML文档中查找信息的语言</p>
<p>XPath的常用规则</p>
<p>我们现用表格列举一下几个常用规则：<br>表达式     描述<br>nodename    选取此节点的所有子节点<br>/   从当前节点选取直接子节点<br>//  从当前节点选取子孙节点<br>.   选取当前节点<br>..  选取当前节点的父节点<br>@   选取属性</p>
<p>在这里列出了XPath的常用匹配规则，例如 / 代表选取直接子节点，// 代表选择所有子孙节点，. 代表选取当前节点，.. 代表选取当前节点的父节点，@ 则是加了属性的限定，选取匹配属性的特定节点。</p>
<p>使用时需要导入lxml库的etree模块，现在用一个实例来感受一下，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = etree.tostring(html)</span><br><span class="line">print(result.decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<p>先导入lxml库的etree模块，然后声明一段HTML文本，调用HTML类进行初始化，这样就构造了一个XPath解析对象，在这里etree模块会将HTML文本自动修正</p>
<p>结果如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;div&gt;</span><br><span class="line">    &lt;ul&gt;</span><br><span class="line">         &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">         &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span><br><span class="line">     &lt;/li&gt;&lt;/ul&gt;</span><br><span class="line"> &lt;/div&gt;</span><br><span class="line">&lt;/body&gt;&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>
<p>也可是使用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br></pre></td></tr></table></figure></p>
<p>直接对文件进行解析</p>
<p>1.所有节点<br>一般使用//开头的规则来选取所有符合要求的节点，<br>对上例中，需要选取所有节点，可以这样实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//*'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure></p>
<p>这会返回一个包含所有节点的列表<br>也可以指点节点名</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li'</span>)</span><br><span class="line">print(result)</span><br><span class="line">print(result[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>2.子节点<br>我们通过 / 或 // 即可查找元素的子节点或子孙节点，加入我们现在想选择 li 节点所有直接 a 子节点，可以这样来实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>在这里我们通过追加一个 /a 即选择了所有 li 节点的所有直接 a 子节点，因为 //li 是选中所有li节点， /a 是选中li节点的所有直接子节点 a，二者组合在一起即获取了所有li节点的所有直接 a 子节点。</p>
<p>3.父节点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//a[@href="link4.html"]/../@class'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>使用..来获取父节点，使用@获取属性</p>
<p>4.属性匹配</p>
<p>在选取的时候我们还可以用 @ 符号进行属性过滤，比如在这里如果我们要选取 class 为 item-1 的 li 节点，可以这样实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>5.文本获取</p>
<p>我们用 XPath 中的 text() 方法可以获取节点中的文本，我们接下来尝试获取一下上文 li 节点中的文本，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]/text()'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>6.属性获取</p>
<p>我们知道了用 text() 可以获取节点内部文本，那么节点属性该怎样获取呢？其实还是用 @ 符号就可以，例如我们想获取所有 li 节点下所有 a 节点的 href 属性，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a/@href'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>时候某些节点的某个属性可能有多个值，例如下面例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="li"]/a/text()'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>这时如果属性有多个值就需要用 contains() 函数了，代码可以改写如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li")]/a/text()'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>多属性时<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first" name="item"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li") and @name="item"]/a/text()'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure></p>
<p>7.按序选择</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">可以利用中括号传入索引的方法获取特定次序的节点，示例如下：</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/a/text()'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[last()]/a/text()'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[position()&lt;3]/a/text()'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[last()-2]/a/text()'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>8.节点轴</p>
<p>XPath 提供了很多节点轴选择方法，英文叫做 XPath Axes，包括获取子元素、兄弟元素、父元素、祖先元素等等，在一定情况下使用它可以方便地完成节点的选择，我们用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;&lt;span&gt;first item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::*'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::div'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/attribute::*'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/child::a[@href="link1.html"]'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/descendant::span'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following::*[2]'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following-sibling::*'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<h4 id="BeautifulSoup的使用"><a href="#BeautifulSoup的使用" class="headerlink" title="BeautifulSoup的使用"></a>BeautifulSoup的使用</h4><p>BeautifulSoup 就是借助网页的结构和属性等特性来解析网页的工具，有了它我们不用再去写一些复杂的正则，只需要简单的1几句就可以完成某些元素的提取</p>
<p>简单来说，BeautifulSoup就是python的一个HTML或XML的解析库，使用它可以十分方便地从网页中提取数据。<br>BS 在解析时，实际上是依赖解析器的 ，<br>解析器     使用方法    优势  劣势<br>Python标准库   BeautifulSoup(markup, “html.parser”)    Python的内置标准库、执行速度适中 、文档容错能力强    Python 2.7.3 or 3.2.2)前的版本中文容错能力差<br>LXML HTML 解析器   BeautifulSoup(markup, “lxml”)   速度快、文档容错能力强     需要安装C语言库<br>LXML XML 解析器    BeautifulSoup(markup, “xml”)    速度快、唯一支持XML的解析器     需要安装C语言库<br>html5lib    BeautifulSoup(markup, “html5lib”)   最好的容错性、以浏览器的方式解析文档、生成 HTML5 格式的文档   速度慢、不依赖外部扩展</p>
<h5 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h5><p>下面我们首先用一个实例来感受一下 BeautifulSoup 的基本使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">html = <span class="string">"""</span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,</span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</span></span><br><span class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.prettify())</span><br><span class="line">print(soup.title.string)</span><br></pre></td></tr></table></figure>
<p>其中初始化时，BeautifulSoup就会帮你完善你HTML代码<br>然后prettify()函数就会将文件以缩进方式排列</p>
<h5 id="使用节点选择器"><a href="#使用节点选择器" class="headerlink" title="使用节点选择器"></a>使用节点选择器</h5><p>即是，直接选择节点的名称，然后调用stirng.导出文本</p>
<p>下面我们再用一个例子详细说明一下它的选择方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">html = <span class="string">"""</span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,</span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</span></span><br><span class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.title)</span><br><span class="line">print(type(soup.title))</span><br><span class="line">print(soup.title.string)</span><br><span class="line">print(soup.head)</span><br><span class="line">print(soup.p)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;</span></span><br><span class="line"><span class="string">&lt;class '</span>bs4.element.Tag<span class="string">'&gt;</span></span><br><span class="line"><span class="string">The Dormouse'</span>s story</span><br><span class="line">&lt;head&gt;&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line">&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>
<h5 id="提取信息"><a href="#提取信息" class="headerlink" title="提取信息"></a>提取信息</h5><p>上面我们直接使用string属性来获取文本的值，那我们要获取节点属性值怎么办？获取节点名怎么办</p>
<p>获取节点名<br>print(soup.title.name)</p>
<p>获取属性<br>每个节点可能有多个属性，比如id，class等等，选择该节点元素之后，就可以调用attrs获取所有属性</p>
<p>print(soup.p.attrs)<br>print(soup.p.attrs[‘name’])</p>
<p>还有一种更简单的获取方式，我们可以不用写 attrs，直接节点元素后面加中括号，传入属性名就可以达到属性值了，样例如下：</p>
<p>print(soup.p[‘name’])<br>print(soup.p[‘class’])</p>
<p>获取内容<br>print(soup.p.string)</p>
<p>嵌套选择<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">html = <span class="string">"""</span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.head.title)</span><br><span class="line">print(type(soup.head.title))</span><br><span class="line">print(soup.head.title.string)</span><br></pre></td></tr></table></figure></p>
<p>关联选择</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">html = <span class="string">"""</span></span><br><span class="line"><span class="string">&lt;html&gt;</span></span><br><span class="line"><span class="string">    &lt;head&gt;</span></span><br><span class="line"><span class="string">        &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></span><br><span class="line"><span class="string">    &lt;/head&gt;</span></span><br><span class="line"><span class="string">    &lt;body&gt;</span></span><br><span class="line"><span class="string">        &lt;p class="story"&gt;</span></span><br><span class="line"><span class="string">            Once upon a time there were three little sisters; and their names were</span></span><br><span class="line"><span class="string">            &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;</span></span><br><span class="line"><span class="string">                &lt;span&gt;Elsie&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &lt;/a&gt;</span></span><br><span class="line"><span class="string">            &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; </span></span><br><span class="line"><span class="string">            and</span></span><br><span class="line"><span class="string">            &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;</span></span><br><span class="line"><span class="string">            and they lived at the bottom of a well.</span></span><br><span class="line"><span class="string">        &lt;/p&gt;</span></span><br><span class="line"><span class="string">        &lt;p class="story"&gt;...&lt;/p&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>子节点<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.p.children)</span><br><span class="line"><span class="keyword">for</span> i, child <span class="keyword">in</span> enumerate(soup.p.children):</span><br><span class="line">    print(i, child)</span><br></pre></td></tr></table></figure></p>
<p>子孙节点：descendants<br>父节点：parent<br>祖先节点：parents<br>兄弟节点：next_sibling    previous_sibling  next_siblings  previous_siblings</p>
<p>信息提取</p>
<p>如果返回结果是单个节点，那么可以直接调用 string、attrs 等属性来获得其文本和属性，如果返回结果是多个节点的生成器，则可以转为列表后取出某个元素，然后再调用 string、attrs 等属性来获取其对应节点等文本和属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Next Sibling:'</span>)</span><br><span class="line">print(type(soup.a.next_sibling))</span><br><span class="line">print(soup.a.next_sibling)</span><br><span class="line">print(soup.a.next_sibling.string)</span><br><span class="line">print(<span class="string">'Parent:'</span>)</span><br><span class="line">print(type(soup.a.parents))</span><br><span class="line">print(list(soup.a.parents)[<span class="number">0</span>])</span><br><span class="line">print(list(soup.a.parents)[<span class="number">0</span>].attrs[<span class="string">'class'</span>])</span><br></pre></td></tr></table></figure>
<h5 id="方法选择器"><a href="#方法选择器" class="headerlink" title="方法选择器"></a>方法选择器</h5><p>find_all()<br>find_all，顾名思义，就是查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，功能十分强大。</p>
<p>它的API如下：</p>
<p>find_all(name , attrs , recursive , text , **kwargs)</p>
<p>我们可以根据节点名来查询元素，下面我们用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">html=<span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt;</span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list" id="list-1"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small" id="list-2"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.find_all(name=<span class="string">'ul'</span>))</span><br><span class="line">print(type(soup.find_all(name=<span class="string">'ul'</span>)[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<p>也可以直接使用soup.find_all(‘ul’)</p>
<p>也可以使用传入属性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(soup.find_all(attrs=&#123;<span class="string">'id'</span>: <span class="string">'list-1'</span>&#125;))</span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">'name'</span>: <span class="string">'elements'</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>于一些常用的属性比如 id、class 等，我们可以不用 attrs 来传递<br>如 id class</p>
<p>text 参数可以用来匹配节点的文本，传入的形式可以是字符串，可以是正则表达式对象<br>print(soup.find_all(text=re.compile(‘link’)))</p>
<p>运行结果：</p>
<p>[‘Hello, this is a link’, ‘Hello, this is a link, too’]</p>
<p>find()方法只返回第一个匹配的元素，而find_all()返回所有匹配的列表</p>
<p>find_parents() find_parent()</p>
<p>find_parents() 返回所有祖先节点，find_parent() 返回直接父节点。<br>find_next_siblings() find_next_sibling()</p>
<p>find_next_siblings() 返回后面所有兄弟节点，find_next_sibling() 返回后面第一个兄弟节点。<br>find_previous_siblings() find_previous_sibling()</p>
<p>find_previous_siblings() 返回前面所有兄弟节点，find_previous_sibling() 返回前面第一个兄弟节点。<br>find_all_next() find_next()</p>
<p>find_all_next() 返回节点后所有符合条件的节点, find_next() 返回第一个符合条件的节点。<br>find_all_previous() 和 find_previous()</p>
<p>find_all_previous() 返回节点后所有符合条件的节点, find_previous() 返回第一个符合条件的节点</p>
<h5 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h5><p>BeautifulSoup 还提供了另一种选择器，CSS选择器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">html=<span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt;</span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list" id="list-1"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small" id="list-2"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.select(<span class="string">'.panel .panel-heading'</span>))</span><br><span class="line">print(soup.select(<span class="string">'ul li'</span>))</span><br><span class="line">print(soup.select(<span class="string">'#list-2 .element'</span>))</span><br><span class="line">print(type(soup.select(<span class="string">'ul'</span>)[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#嵌套</span></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul.select(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取属性</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul[<span class="string">'id'</span>])</span><br><span class="line">    print(ul.attrs[<span class="string">'id'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取文本</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> soup.select(<span class="string">'li'</span>):</span><br><span class="line">    print(<span class="string">'Get Text:'</span>, li.get_text())</span><br><span class="line">    print(<span class="string">'String:'</span>, li.string)</span><br></pre></td></tr></table></figure>
<h4 id="PyQuery的使用"><a href="#PyQuery的使用" class="headerlink" title="PyQuery的使用"></a>PyQuery的使用</h4><p>PyQuery 和BeautifulSoup类似，但是，其CSS选择器更加强大</p>
<h5 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h5><p>PyQuery 需要传入HTML来进行初始化，可以传入字符串，<br>URL，文件名<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#字符串初始化</span></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">print(doc(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#URL初始化</span></span><br><span class="line">doc = pq(url=<span class="string">'http://cuiqingcai.com'</span>)</span><br><span class="line">print(doc(<span class="string">'title'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件初始化</span></span><br><span class="line">doc = pq(filename=<span class="string">'demo.html'</span>)</span><br><span class="line">print(doc(<span class="string">'li'</span>))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div id="container"&gt;</span></span><br><span class="line"><span class="string">    &lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">print(doc(<span class="string">'#container .list li'</span>))</span><br><span class="line">print(type(doc(<span class="string">'#container .list li'</span>)))</span><br></pre></td></tr></table></figure>
<h5 id="基本CSS选择器"><a href="#基本CSS选择器" class="headerlink" title="基本CSS选择器"></a>基本CSS选择器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div id="container"&gt;</span></span><br><span class="line"><span class="string">    &lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">print(doc(<span class="string">'#container .list li'</span>))</span><br><span class="line">print(type(doc(<span class="string">'#container .list li'</span>)))</span><br><span class="line"><span class="comment">#container .list li 意思是选择id为container 的节点内部所有li节点</span></span><br></pre></td></tr></table></figure>
<h5 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h5><p>使用find()函数，传入CSS选择器，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查找子节点</span></span><br><span class="line">doc = pq(html)</span><br><span class="line">items = doc(<span class="string">'.list'</span>)</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">lis = items.find(<span class="string">'li'</span>)</span><br><span class="line">print(type(lis))</span><br><span class="line">print(lis)</span><br><span class="line"></span><br><span class="line"><span class="comment">#find查找所有的子孙节点，用children查找子节点</span></span><br><span class="line">lis = items.children()</span><br><span class="line">print(type(lis))</span><br><span class="line">print(lis)</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果要筛选符合条件的，可以传入CSS选择器</span></span><br><span class="line">lis = items.children(<span class="string">'.active'</span>)</span><br><span class="line">print(lis)</span><br></pre></td></tr></table></figure>
<p>查找父节点<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;/ul&gt;</span></span><br><span class="line"><span class="string">     &lt;/div&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">items = doc(<span class="string">'.list'</span>)</span><br><span class="line">container = items.parent()</span><br><span class="line">print(type(container))</span><br><span class="line">print(container)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用parents查找祖先节点</span></span><br><span class="line">doc = pq(html)</span><br><span class="line">items = doc(<span class="string">'.list'</span>)</span><br><span class="line">parents = items.parents()</span><br><span class="line">print(type(parents))</span><br><span class="line">print(parents)</span><br><span class="line"></span><br><span class="line">parent = items.parents(<span class="string">'.wrap'</span>)</span><br><span class="line">print(parent)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#兄弟节点</span></span><br><span class="line"></span><br><span class="line">li = doc(<span class="string">'.list .item-0.active'</span>)</span><br><span class="line">print(li.siblings())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">li = doc(<span class="string">'.list .item-0.active'</span>)</span><br><span class="line">print(li.siblings(<span class="string">'.active'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#节点遍历</span></span><br><span class="line">lis = doc(<span class="string">'li'</span>).items()</span><br><span class="line">print(type(lis))</span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> lis:</span><br><span class="line">    print(li,type(li))</span><br></pre></td></tr></table></figure></p>
<h5 id="获取信息"><a href="#获取信息" class="headerlink" title="获取信息"></a>获取信息</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;/ul&gt;</span></span><br><span class="line"><span class="string">     &lt;/div&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">a = doc(<span class="string">'.item-0.active a'</span>)</span><br><span class="line">print(a,type(a))</span><br><span class="line">print(a.attr(<span class="string">'href'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取文本</span></span><br><span class="line">print(a.text())</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取所有HTML文本</span></span><br><span class="line">print(a.html())</span><br></pre></td></tr></table></figure>
<h5 id="节点操作"><a href="#节点操作" class="headerlink" title="节点操作"></a>节点操作</h5><p>addClass,removeClass<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;/ul&gt;</span></span><br><span class="line"><span class="string">     &lt;/div&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">   '''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">li = doc(<span class="string">'.liem-0.active'</span>)</span><br><span class="line">print(li)</span><br><span class="line">li.removeClass(<span class="string">'active'</span>)</span><br><span class="line">print(li)</span><br><span class="line">li.addClass(<span class="string">'active'</span>)</span><br><span class="line">print(li)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#运行结果</span></span><br><span class="line">&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">&lt;li class="item-0"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span><br></pre></td></tr></table></figure></p>
<p>attr,text,html</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">     &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;/ul&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">li = doc(.item<span class="number">-0.</span>active)</span><br><span class="line">print(li)</span><br><span class="line">li.attr(<span class="string">'name'</span>,<span class="string">'link'</span>)</span><br><span class="line">print(<span class="string">'li'</span>)</span><br><span class="line">li.text(<span class="string">'changed item'</span>)</span><br><span class="line">print(li)</span><br><span class="line">li.html(<span class="string">'&lt;span&gt;changes item&lt;/span&gt;'</span>)</span><br><span class="line">print(li)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">在这里我们首先选中了 li 节点，然后调用 attr() 方法来修改属性，第一个参数为属性名，第二个参数为属性值，然后我们调用了 text() 和 html() 方法来改变节点内部的内容。三次操作后分别又打印输出当前 li 节点。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#运行结果：</span></span><br><span class="line"></span><br><span class="line">&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">&lt;li class="item-0 active" name="link"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">&lt;li class="item-0 active" name="link"&gt;changed item&lt;/li&gt;</span><br><span class="line">&lt;li class="item-0 active" name="link"&gt;&lt;span&gt;changed item&lt;/span&gt;&lt;/li&gt;</span><br></pre></td></tr></table></figure>
<p>remove<br>emove 顾名思义移除，remove() 方法有时会为信息的提取带来非常大的便利。下面我们看一个实例：<br>‘’’python<br>html = ‘’’</p>
<p><div calss="wrap"><br>    Hello,World<br>    <p>This is a paragraph.</p><br></div><br>‘’’</p>
<p>from pyquery import PyQuery as pq<br>doc = pq(“.wrap”)<br>print(wrap.text())</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 伪类选择器</span><br><span class="line"></span><br><span class="line">例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等等，我们用一个实例感受一下：</span><br><span class="line">```python</span><br><span class="line">html = &apos;&apos;&apos;</span><br><span class="line">&lt;div class=&quot;wrap&quot;&gt;</span><br><span class="line">    &lt;div id=&quot;container&quot;&gt;</span><br><span class="line">        &lt;ul class=&quot;list&quot;&gt;</span><br><span class="line">             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;</span><br><span class="line">             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">         &lt;/ul&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line"> &lt;/div&gt;</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">li = doc(&apos;li:first-child&apos;)</span><br><span class="line">print(li)</span><br><span class="line">li = doc(&apos;li:last-child&apos;)</span><br><span class="line">print(li)</span><br><span class="line">li = doc(&apos;li:nth-child(2)&apos;)</span><br><span class="line">print(li)</span><br><span class="line">li = doc(&apos;li:gt(2)&apos;)</span><br><span class="line">print(li)</span><br><span class="line">li = doc(li:nth-child(2n))</span><br><span class="line">print(li)</span><br><span class="line">li = doc(&apos;li:contains(second)&apos;)</span><br><span class="line">print(li)</span><br></pre></td></tr></table></figure>
<h3 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h3><p>用解析器将数据解析出来之后，就是需要对文件进行存储了，最简单的是直接保存为文件，如TXT,json,CSV deng,也能保存到数据中，如关系型数据库MySQL，非关系型数据库MongoDB,Redis等等。</p>
<h4 id="文件存储-1"><a href="#文件存储-1" class="headerlink" title="文件存储"></a>文件存储</h4><p>txt存储<br>打开方式</p>
<pre><code>r，以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。
rb，以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。
r+，打开一个文件用于读写。文件指针将会放在文件的开头。
rb+，以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。
w，打开一个文件只用于写入。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。
wb ，以二进制格式打开一个文件只用于写入。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。
w+， 打开一个文件用于读写。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。
wb+，以二进制格式打开一个文件用于读写。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。
a，打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。
a+，打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。
ab+，以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。
</code></pre><p>简化写法<br>with open(‘explore.txt’, ‘a’, encoding=’utf-8’) as file:<br>    file.write(‘\n’.join([question, author, answer]))<br>    file.write(‘\n’ + ‘=’ * 50 + ‘\n’)</p>
<p>Json<br>一个 Json 对象可以写为如下形式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[&#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"Bob"</span>,</span><br><span class="line">    <span class="string">"gender"</span>: <span class="string">"male"</span>,</span><br><span class="line">    <span class="string">"birthday"</span>: <span class="string">"1992-10-18"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">     <span class="string">"name"</span>: <span class="string">"Selina"</span>,</span><br><span class="line">    <span class="string">"gender"</span>: <span class="string">"female"</span>,</span><br><span class="line">    <span class="string">"birthday"</span>: <span class="string">"1995-10-18"</span></span><br><span class="line">&#125;]</span><br></pre></td></tr></table></figure></p>
<p>读取Json<br>使用loads方法</p>
<p>data = json.loads(str)</p>
<p>with open(‘data.json’, ‘w’) as file:<br>    file.write(json.dumps(data))</p>
<p>还能将字符串重新写入文本</p>
<p>另外如果我们想保存 Json 的格式，可以再加一个参数 indent，代表缩进字符个数。</p>
<p>with open(‘data.json’, ‘w’) as file:<br>    file.write(json.dumps(data, indent=2))</p>
<p>写入中文时要注意<br>with open(‘data.json’, ‘w’, encoding=’utf-8’) as file:<br>    file.write(json.dumps(data, indent=2, ensure_ascii=False))</p>
<p>CSV文件存储</p>
<p>1.写入</p>
<p>在这里我们先看一个最简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.csv'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    writer = csv.writer(csvfile)</span><br><span class="line">    writer.writerow([<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'age'</span>])</span><br><span class="line">    writer.writerow([<span class="string">'10001'</span>, <span class="string">'Mike'</span>, <span class="number">20</span>])</span><br><span class="line">    writer.writerow([<span class="string">'10002'</span>, <span class="string">'Bob'</span>, <span class="number">22</span>])</span><br><span class="line">    writer.writerow([<span class="string">'10003'</span>, <span class="string">'Jordan'</span>, <span class="number">21</span>])</span><br></pre></td></tr></table></figure>
<p>默认使用逗号分隔符，也可以使用delimiter参数，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data.csv'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    writer = csv.writer(csvfile, delimiter=<span class="string">' '</span>)</span><br><span class="line">    writer.writerow([<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'age'</span>])</span><br><span class="line">    writer.writerow([<span class="string">'10001'</span>, <span class="string">'Mike'</span>, <span class="number">20</span>])</span><br><span class="line">    writer.writerow([<span class="string">'10002'</span>, <span class="string">'Bob'</span>, <span class="number">22</span>])</span><br><span class="line">    writer.writerow([<span class="string">'10003'</span>, <span class="string">'Jordan'</span>, <span class="number">21</span>])</span><br></pre></td></tr></table></figure>
<p>另外我们也可以调用 writerows() 方法同时写入多行，此时参数就需要为二维列表，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.csv'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    writer = csv.writer(csvfile)</span><br><span class="line">    writer.writerow([<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'age'</span>])</span><br><span class="line">    writer.writerows([[<span class="string">'10001'</span>, <span class="string">'Mike'</span>, <span class="number">20</span>], [<span class="string">'10002'</span>, <span class="string">'Bob'</span>, <span class="number">22</span>], [<span class="string">'10003'</span>, <span class="string">'Jordan'</span>, <span class="number">21</span>]])</span><br></pre></td></tr></table></figure>
<p>输出效果是相同的，内容如下：<br>id,name,age<br>10001,Mike,20<br>10002,Bob,22<br>10003,Jordan,21</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    fieldnames = [<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'age'</span>]</span><br><span class="line">    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)</span><br><span class="line">    writer.writerow(&#123;<span class="string">'id'</span>: <span class="string">'10005'</span>, <span class="string">'name'</span>: <span class="string">'王伟'</span>, <span class="string">'age'</span>: <span class="number">22</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>读取<br>我们同样可以使用 csv 库来读取 CSV 文件，例如我们现在将刚才写入的文件内容读取出来，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.csv'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    reader = csv.reader(csvfile)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">        print(row)</span><br></pre></td></tr></table></figure>
<p>接触过 Pandas 的话，可以利用 read_csv() 方法将数据从 CSV 中读取出来，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas  <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'data.csv'</span>)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
<h5 id="MySQL数据库"><a href="#MySQL数据库" class="headerlink" title="MySQL数据库"></a>MySQL数据库</h5><p>比较简单就是使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">cursor.execute(<span class="string">'SELECT VERSION()'</span>)</span><br><span class="line">data = cursor.fetchone()</span><br><span class="line">print(<span class="string">'Database version:'</span>, data)</span><br><span class="line">cursor.execute(<span class="string">"CREATE DATABASE spiders DEFAULT CHARACTER SET utf8"</span>)</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure>
<p>首先连接，然后使用cursor()获取Mysql的操作游标，利用游标来执行SQL语句</p>
<p>插入数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sql = <span class="string">'INSERT INTO students(id, name, age) values(%s, %s, %s)'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql, (id, user, age))</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure></p>
<p>注意数据的增删查改必须使用commit()来实行</p>
<p>值得一提，数据的修改是具有事务问题，事务具有四个性质</p>
<p>原子性（atomicity）  一个事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做。<br>一致性（consistency）    事务必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。<br>隔离性（isolation）  一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。<br>持久性（durability）     持续性也称永久性（permanence），指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。</p>
<h3 id="Ajax数据爬取"><a href="#Ajax数据爬取" class="headerlink" title="Ajax数据爬取"></a>Ajax数据爬取</h3><h4 id="什么Ajax"><a href="#什么Ajax" class="headerlink" title="什么Ajax"></a>什么Ajax</h4><p>Ajax 全称Asynchronous JavaScript and XML<br>即异步的JavaScript和XML</p>
<p>Ajax 不是一门编程语言，而是利用js在保证页面不更新，页面链接不改的情况下和服务器交换并更新部分网页的技术</p>
<h5 id="基本原理-1"><a href="#基本原理-1" class="headerlink" title="基本原理"></a>基本原理</h5><p>可以简单分为三步<br>发送请求<br>解析内容<br>渲染网页</p>
<p>发送请求，</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/03/regex/" rel="next" title="regex">
                <i class="fa fa-chevron-left"></i> regex
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Yao</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫基础"><span class="nav-number">1.</span> <span class="nav-text">爬虫基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#爬虫基本原理"><span class="nav-number">1.1.</span> <span class="nav-text">爬虫基本原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Session-和-Cookies"><span class="nav-number">1.2.</span> <span class="nav-text">Session 和 Cookies</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#会话维持"><span class="nav-number">1.2.1.</span> <span class="nav-text">会话维持</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Cookie"><span class="nav-number">1.2.2.</span> <span class="nav-text">Cookie</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代理基本定理"><span class="nav-number">1.3.</span> <span class="nav-text">代理基本定理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#基本原理"><span class="nav-number">1.3.1.</span> <span class="nav-text">基本原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#代理作用"><span class="nav-number">1.3.2.</span> <span class="nav-text">代理作用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基本库的使用"><span class="nav-number">2.</span> <span class="nav-text">基本库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#使用urllib"><span class="nav-number">2.1.</span> <span class="nav-text">使用urllib</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#request"><span class="nav-number">2.1.1.</span> <span class="nav-text">request</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#urllib-处理异常"><span class="nav-number">2.1.2.</span> <span class="nav-text">urllib 处理异常</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#解析连接"><span class="nav-number">2.1.3.</span> <span class="nav-text">解析连接</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#分析robot协议"><span class="nav-number">2.1.4.</span> <span class="nav-text">分析robot协议</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用requests"><span class="nav-number">2.2.</span> <span class="nav-text">使用requests</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#简单使用"><span class="nav-number">2.2.1.</span> <span class="nav-text">简单使用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#高级使用"><span class="nav-number">2.2.2.</span> <span class="nav-text">高级使用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#re模块使用正则表达式"><span class="nav-number">2.2.3.</span> <span class="nav-text">re模块使用正则表达式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解析库"><span class="nav-number">3.</span> <span class="nav-text">解析库</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#XPath"><span class="nav-number">3.1.</span> <span class="nav-text">XPath</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BeautifulSoup的使用"><span class="nav-number">3.2.</span> <span class="nav-text">BeautifulSoup的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#基本使用"><span class="nav-number">3.2.1.</span> <span class="nav-text">基本使用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#使用节点选择器"><span class="nav-number">3.2.2.</span> <span class="nav-text">使用节点选择器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#提取信息"><span class="nav-number">3.2.3.</span> <span class="nav-text">提取信息</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#方法选择器"><span class="nav-number">3.2.4.</span> <span class="nav-text">方法选择器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CSS选择器"><span class="nav-number">3.2.5.</span> <span class="nav-text">CSS选择器</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PyQuery的使用"><span class="nav-number">3.3.</span> <span class="nav-text">PyQuery的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#初始化"><span class="nav-number">3.3.1.</span> <span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#基本CSS选择器"><span class="nav-number">3.3.2.</span> <span class="nav-text">基本CSS选择器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#查找"><span class="nav-number">3.3.3.</span> <span class="nav-text">查找</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#获取信息"><span class="nav-number">3.3.4.</span> <span class="nav-text">获取信息</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#节点操作"><span class="nav-number">3.3.5.</span> <span class="nav-text">节点操作</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文件存储"><span class="nav-number">4.</span> <span class="nav-text">文件存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#文件存储-1"><span class="nav-number">4.1.</span> <span class="nav-text">文件存储</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#MySQL数据库"><span class="nav-number">4.1.1.</span> <span class="nav-text">MySQL数据库</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ajax数据爬取"><span class="nav-number">5.</span> <span class="nav-text">Ajax数据爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么Ajax"><span class="nav-number">5.1.</span> <span class="nav-text">什么Ajax</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#基本原理-1"><span class="nav-number">5.1.1.</span> <span class="nav-text">基本原理</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yao</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
